# -*- coding: utf-8 -*-
"""MetArtClassifier2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JfwZEtLR1g9VKJcSFa01Kdj4z1_hBpkG
"""

import numpy as np
import pandas as pd
import random
import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import tensorflow as tf

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
images = [f'/content/drive/MyDrive/Art Classifier/Images/Processed/{i}' for i in
os.listdir('/content/drive/MyDrive/Art Classifier/Images/Processed')]
plt.figure(figsize=(20, 10))
cols = 5
for i in range(cols):
  plt.subplot(int(5 / cols+1), cols, i+1)
  plt.imshow(mpimg.imread(images[i]))

def filter_images(imgs, metadata, filter_column):
  import pandas as pd
  meta = pd.read_csv(metadata)
  df = pd.DataFrame(meta)
  #print(df.head())
  image_ids = set()

  for fnam in imgs[0:len(imgs)]:
    #print(fnam)
    fnam_rep = float(fnam.replace('.jpg', '').replace(' (1)',''))
    #print(fnam_rep)
    for index, row in df.iterrows():
      #print(row)
      #print(row['object_id'])
      if row["object_id"] == fnam_rep and row[filter_column] != "Unknown":
        image_ids.add(row["object_id"])
  return image_ids

def filter_images_optimized(imgs, metadata, filter_column):
    import pandas as pd

    # Read CSV once
    df = pd.read_csv(metadata)

    # Extract object IDs from filenames (vectorized)
    object_ids = []
    for fnam in imgs:
        fnam_rep = float(fnam.replace('.jpg', '').replace(' (1)', ''))
        object_ids.append(fnam_rep)

    # Convert to set for O(1) lookup
    object_ids_set = set(object_ids)

    # Filter dataframe efficiently
    filtered_df = df[
        (df['object_id'].isin(object_ids_set)) &
        (df[filter_column] != "Unknown")
    ]

    # Return as set
    return set(filtered_df['object_id'].values)

train_dir = '/content/drive/MyDrive/Art Classifier/Images/Train'
val_dir = '/content/drive/MyDrive/Art Classifier/Images/Validation'
test_dir = '/content/drive/MyDrive/Art Classifier/Images/Test'
metadata = '/content/drive/MyDrive/Art Classifier/MetObjFiltered.csv'

unpack_dir = '/content/drive/MyDrive/Art Classifier/Images/Processed'
tr_imgs = [f'{i}' for i in os.listdir(unpack_dir)]

np.random.shuffle(tr_imgs)

trainratio = 0.75
valratio = 0.1

image_ids_to_move = filter_images_optimized(tr_imgs, metadata, 'Century_binary')
print(len(image_ids_to_move))
#print(image_ids_to_move[0:5])

train = int(len(image_ids_to_move) * trainratio)
val = int(len(image_ids_to_move) * valratio)
print(train, val)

counter = 1
train_list = []
val_list = []
test_list = []
for i in image_ids_to_move:
  if counter <= train:
    train_list.append(i)
  elif counter <= train + val:
    val_list.append(i)
  else:
    test_list.append(i)
  counter += 1

print(len(train_list))
print(len(val_list))
print(len(test_list))

def filter_metadata(image_ids, metadata):
  import pandas as pd

  # Read CSV once
  df = pd.read_csv(metadata)

  # Filter dataframe efficiently
  filtered_df = df[(df['object_id'].isin(image_ids))]
  return filtered_df

train_metadata = filter_metadata(train_list, metadata)
val_metadata = filter_metadata(val_list, metadata)
test_metadata = filter_metadata(test_list, metadata)

train_metadata.to_csv('/content/drive/MyDrive/Art Classifier/train_metadata.csv')
val_metadata.to_csv('/content/drive/MyDrive/Art Classifier/val_metadata.csv')
test_metadata.to_csv('/content/drive/MyDrive/Art Classifier/test_metadata.csv')

def move_images(imgs, source, destination, image_ids):

  for fnam in imgs:
    fnam_rep = (fnam, float(fnam.replace('.jpg', '').replace(' (1)', '')))
    for i in image_ids:
      if fnam_rep[1] == i:
        srcfile = source + '/' + fnam_rep[0]
        destfile = destination + '/' + fnam_rep[0]
        os.replace(srcfile, destfile)

# move_images(tr_imgs, unpack_dir, train_dir, train_list)

df_test = pd.read_csv('/content/drive/MyDrive/Art Classifier/test_metadata.csv')
#df_test.head()
test_list2 = df_test['object_id'].values
print(test_list2)

#move_images(tr_imgs, unpack_dir, val_dir, val_list)
move_images(tr_imgs, unpack_dir, test_dir, test_list2)

print('Paintings in training set ', len(os.listdir(train_dir)))
print('Paintings in testing set ', len(os.listdir(test_dir)))
print('Paintings in validation set ', len(os.listdir(val_dir)))

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os # Import os module

# Prepare your dataframes for each split
def prepare_dataframe(image_dir, metadata, class_column):
    # Get list of actual image files
    image_files = [f for f in os.listdir(image_dir)
                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    df = pd.read_csv(metadata)

    # Create dataframe with filename and class
    data = []
    print(f"--- Debugging prepare_dataframe ---") # Debug print
    print(f"Processing directory: {image_dir}") # Debug print
    print(f"Found {len(image_files)} image files in directory.") # Debug print
    print(f"Metadata file used: {metadata}") # Debug print
    print(f"Metadata shape: {df.shape}") # Debug print
    print(f"Metadata columns: {df.columns.tolist()}") # Debug print
    if 'object_id' not in df.columns:
        print("Error: 'object_id' column not found in metadata.") # Debug print
        return pd.DataFrame(data) # Return empty if essential column is missing


    for i, img_file in enumerate(image_files):
        if i < 20: # Limit debug prints to the first 20 files
            print(f"\n--- Processing file {i+1}/{len(image_files)}: {img_file}") # Debug print

        object_id_str = os.path.splitext(img_file)[0].replace(' (1)', '') # adjust based on your naming

        # Handle potential errors if object_id is not found or not a valid number
        try:
            object_id_float = float(object_id_str)
            if i < 20: print(f"Extracted object_id_float: {object_id_float}") # Debug print
        except ValueError:
            if i < 20: print(f"Skipping file due to ValueError in object_id: {img_file}") # Debug print
            continue  # Skip if object_id cannot be converted to float

        # Check if the extracted object_id_float is in the metadata DataFrame
        # Use .any() for efficiency and clarity when checking existence in a Series
        if (df['object_id'] == object_id_float).any():
            if i < 20: print(f"Object ID {object_id_float} found in metadata.") # Debug print
            # Retrieve the class label using boolean indexing
            class_label = df[df['object_id'] == object_id_float][class_column].iloc[0]
            data.append({'filename': img_file, 'class': str(class_label)})
            if i < 20: print(f"Appended data: {{'filename': '{img_file}', 'class': '{class_label}'}}") # Debug print
        else:
            if i < 20: print(f"Object ID {object_id_float} not found in metadata.") # Debug print


    print(f"\n--- Finished processing directory: {image_dir}. Collected {len(data)} data entries. ---") # Debug print
    return pd.DataFrame(data)

# Prepare dataframes
train_df = prepare_dataframe(train_dir,'/content/drive/MyDrive/Art Classifier/train_metadata.csv', "Century_binary")
print(train_df.head())

test_df = prepare_dataframe(test_dir, '/content/drive/MyDrive/Art Classifier/test_metadata.csv', "Century_binary")
val_df = prepare_dataframe(val_dir, '/content/drive/MyDrive/Art Classifier/val_metadata.csv', "Century_binary")

print(val_df.head())

print(test_df)

datagen = ImageDataGenerator(
    rescale=1./255,
    # Add other augmentations for training if needed
)

train_generator = datagen.flow_from_dataframe(
    train_df,
    directory=train_dir,
    x_col='filename',
    y_col='class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'  # or 'sparse' if you prefer integer labels
)

test_generator = datagen.flow_from_dataframe(
    test_df,
    directory=test_dir,
    x_col='filename',
    y_col='class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'  # or 'sparse' if you prefer integer labels
)

val_generator = datagen.flow_from_dataframe(
    val_df,
    directory=val_dir,
    x_col='filename',
    y_col='class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'  # or 'sparse' if you prefer integer labels
)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing.image import img_to_array, load_img
"""
batch_size = 32

trn_data = ImageDataGenerator(
            rotation_range=40,
            width_shift_range=0.0,
            height_shift_range=0.0,
            shear_range=0.0,
            zoom_range=0.0,
            horizontal_flip=False)
valtst_data = ImageDataGenerator()

trn_gen = trn_data.flow_from_directory(train_dir, batch_size=batch_size,
class_mode='binary')
val_gen = valtst_data.flow_from_directory(val_dir, batch_size=batch_size,
class_mode='binary')
tst_gen = valtst_data.flow_from_directory(test_dir, batch_size=batch_size,
class_mode='binary', shuffle=False)
"""

from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2
from tensorflow.keras import layers, models
def gen_model():
  base=InceptionResNetV2(weights='imagenet',
  include_top=False,
  input_shape=(224, 224, 3))
  model = models.Sequential()
  model.add(base)
  model.add(layers.Flatten())
  model.add(layers.Dense(256, activation='relu'))
  model.add(layers.Dense(2, activation='softmax'))
  base.trainable = False
  return model

model = gen_model()
model.summary()

from tensorflow.keras import optimizers
epochs = 1
model.compile(loss='sparse_categorical_crossentropy',
optimizer=optimizers.Adam(learning_rate=2e-5),
metrics=['acc'])
hist = model.fit(train_generator, validation_data=val_generator,
epochs=epochs)

pr_yclass = model.predict(test_generator)
print(pr_yclass)
pr_y = np.argmax(pr_yclass, axis=1)
true_y = test_generator.classes

print("Accuracy ",(pr_y==true_y).sum()/len(true_y))
tf.math.confusion_matrix(true_y, pr_y)

badresults = np.where((pr_y != true_y))
cols = 5
col = 1
rows = int(len(badresults[0])/cols)+1
print("rows =",rows,"cols =",cols)
plt.figure(figsize=(20, 50))
for i in badresults[0]:
    plt.subplot(rows, cols, col)
    plt.imshow(mpimg.imread('/content/drive/MyDrive/Art Classifier/Images/Test/' +
test_generator.filenames[i]))
    col += 1

from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras import layers, models

image_size = 224

def gen_modelVGG():
  base=VGG16(weights='imagenet',
  include_top=False,
  input_shape=(image_size, image_size, 3))
  model = models.Sequential()
  model.add(base)
  model.add(layers.Flatten())
  model.add(layers.Dense(256, activation='relu'))
  model.add(layers.Dense(2, activation='softmax'))
  base.trainable = False
  return model

model_VGG = gen_modelVGG()
model_VGG.summary()

input_shape=(224, 224, 3)

from tensorflow.keras import optimizers
epochs = 1
model_VGG.compile(loss='sparse_categorical_crossentropy',
optimizer=optimizers.Adam(learning_rate=2e-5),
metrics=['acc'])
hist = model_VGG.fit(train_generator, validation_data=val_generator,
epochs=epochs)

pr_yclass = model_VGG.predict(test_generator)
print(pr_yclass)
pr_y = np.argmax(pr_yclass, axis=1)
true_y = test_generator.classes

print("Accuracy ",(pr_y==true_y).sum()/len(true_y))
tf.math.confusion_matrix(true_y, pr_y)

print("First 5 rows of test_df:")
display(test_df.head())

print("\nFirst 10 files in test_dir:")
try:
    files_in_test_dir = os.listdir(test_dir)
    # Filter to show only image files for easier comparison
    image_files_in_test_dir = [f for f in files_in_test_dir if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    if image_files_in_test_dir:
        for i, filename in enumerate(image_files_in_test_dir[:10]):
            print(filename)
    else:
        print("No image files found in test_dir.")
except FileNotFoundError:
    print(f"Error: The directory {test_dir} was not found.")
except Exception as e:
    print(f"An error occurred while listing files: {e}")

import os
import pandas as pd

test_dir = '/content/drive/MyDrive/Art Classifier/Images/Test'
test_metadata_path = '/content/drive/MyDrive/Art Classifier/test_metadata.csv'

print(f"Checking files in directory: {test_dir}")
print(f"Checking against metadata file: {test_metadata_path}")

try:
    # List image files in the test directory
    image_files_in_test_dir = [f for f in os.listdir(test_dir)
                               if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    print(f"Found {len(image_files_in_test_dir)} image files in {test_dir}.")

    if not image_files_in_test_dir:
        print("No image files found in the test directory. This is likely the reason the generator finds 0 images.")

    # Read the test metadata
    try:
        test_meta_df = pd.read_csv(test_metadata_path)
        print(f"Read metadata with {test_meta_df.shape[0]} rows.")
        if 'object_id' not in test_meta_df.columns:
             print("Error: 'object_id' column not found in test_metadata.csv")
        else:
            metadata_object_ids = set(test_meta_df['object_id'].values)
            print(f"Extracted {len(metadata_object_ids)} unique object IDs from metadata.")

            # Check if object IDs from directory filenames are in metadata
            matched_count = 0
            unmatched_files = []
            print("\nChecking first 20 files in test_dir against metadata:")
            for i, filename in enumerate(image_files_in_test_dir):
                if i < 20:
                    print(f"  Processing file: {filename}")

                object_id_str = os.path.splitext(filename)[0].replace(' (1)', '')

                try:
                    object_id_float = float(object_id_str)
                    if i < 20: print(f"    Extracted object_id_float: {object_id_float}")

                    if object_id_float in metadata_object_ids:
                        matched_count += 1
                        if i < 20: print(f"    Match found in metadata.")
                    else:
                        unmatched_files.append(filename)
                        if i < 20: print(f"    No match found in metadata.")

                except ValueError:
                    if i < 20: print(f"    Skipping file due to ValueError in object_id: {filename}")
                    unmatched_files.append(filename) # Consider files with invalid names as unmatched


            print(f"\nSummary of checks:")
            print(f"Files in {test_dir}: {len(image_files_in_test_dir)}")
            print(f"Object IDs in {test_metadata_path}: {len(metadata_object_ids)}")
            print(f"Files in {test_dir} with matching object_id in metadata: {matched_count}")
            print(f"Files in {test_dir} without matching object_id in metadata: {len(unmatched_files)}")

            if len(image_files_in_test_dir) > 0 and matched_count == 0:
                 print("\nCrucially, no image files in the test directory had a matching object_id in the test metadata.")
                 print("This confirms the mismatch is the reason the generator finds 0 images.")
                 print("You need to ensure the files in the test directory correspond to the object IDs in the test_metadata.csv file.")


    except FileNotFoundError:
        print(f"Error: Metadata file not found at {test_metadata_path}")
    except Exception as e:
        print(f"An error occurred while reading metadata or checking matches: {e}")

except FileNotFoundError:
    print(f"Error: The test directory was not found at {test_dir}")
except Exception as e:
    print(f"An error occurred while listing files in the test directory: {e}")